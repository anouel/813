---
title: "Questions about logit"
output:
  html_document:
    #code_folding: hide
    toc: true
    toc_float: true
---


```{r setup, include=FALSE}
# Sets defaults for R chunks
knitr::opts_chunk$set(echo = TRUE, # echo = TRUE means that your code will show
                      warning=FALSE,
                      message=FALSE,
                      # fig.path='Figs/', # where to save figures
                      fig.height = 3,
                      fig.width = 4,
                      fig.align = 'center')

# Add any R packages you require. 
# Here are some we will use in 811:
requires <- c("tidyverse", # tidyverse includes dplyr and ggplot2
              "broom",
              "haven",
              "devtools",
              "magrittr",
              "mvtnorm",
              "here")

# Install any you don't have
to_install <- c(!requires %in% rownames(installed.packages()))
install.packages(c(requires[to_install], "NA"), repos = "https://cloud.r-project.org/" )

# Load all required R packages
library(tidyverse)
library(broom)
library(haven)
library(magrittr)
library(here)
library(ggplot2); theme_set(theme_bw()) # global plot theme
# Colorblind and greyscale print-friendly scales
  options(
    ggplot2.continuous.color = "viridis",
    ggplot2.continuous.fill = "viridis"
  )
  scale_color_discrete <- function(...)
    scale_color_viridis_d(..., begin = 0, end = .7)
  scale_fill_discrete <- function(...)
    scale_fill_viridis_d(..., begin = 0, end = .7)
```

# How do we model logit in R?
## Logit is just a linear model predicting log odds. 

```{r model}
d <- read_dta("data/EX4.dta") %>% zap_formats() 

model <- glm(Probat ~ Take + Report + Night + Convict, 
             data=d, 
             family=binomial(link="logit"))

knitr::kable(tidy(model), digits = 3)
```

# How do we interpret a logit?
Interpreting log odds, however, is tricky. Thus we transform the estimated log odds into predicted probabilities at relevant potential values of our variables. 

## Predicted probability
```{r augment, fig.height=4}
# A data frame of values at which to estimate probabilities:
values <- d %>% 
  expand(Take = mean(Take), Report, Night = 1, Convict)

# augment() gives us predicted outcomes and standard errors 
# The new data argument specifies values at which we want to predict
# type.predict applies the link function to the fitted values
predicted <- augment(model, newdata = values, type.predict = "response") 

# Naming things a bit better
predicted %<>% 
  mutate(Convict = str_c(Convict, " Conviction(s)")) %>%
  rename(Convictions = Convict) %>% 
  mutate(Report = ifelse(Report == 1, "Report", "No Report"))

# As a table
predicted %>% 
  select(Report, Convictions, .fitted) %>%
  mutate(.fitted = round(.fitted, 1)) %>% 
  group_by(Convictions, Report) %>%
  spread(key = Report, value = ".fitted") %>% 
  knitr::kable(caption = "Probability of probation")

# As a plot
predicted %>% 
  ggplot() + 
  aes(x = factor(Report), y = .fitted) + 
  geom_point(alpha = .5) +
  geom_pointrange(aes(ymin = .fitted - 1.96*.se.fit,
                  ymax = .fitted + 1.96*.se.fit) )  + 
  coord_flip() +
  facet_wrap("Convictions", ncol = 1) +
  labs(y = "Probability", x = "")
```

# How can we calculate the uncertainty in these estimates?

Instead of relying on functions like `augment`, `Zelig`, or `margins`, we can estimate uncertainty more directly by sampling the assumed distribution of our parameters.

The central limit theorem shows that, with a large enough sample and bounded variance, we can simulate the distribution of parameters with draws from a multivariate normal distribution.

Consider draws of 10, 100, and 1000 for one paramater:
```{r simple-multi, fig.height=4, fig.width=8}
draw <- function(n){
    # rnorm(n) returns n draws from a normal with a given mean and sd,
  # mvrnorm(n) returns n draws from a multivariate normal
  # given a vector of means (i.e. our betas) and sigma, the covariance
rmvnorm(n = n, 
        mean = model$coefficients, 
        sigma = vcov(model)) %>%
    as_tibble() %>% 
    rename_all(~str_c("beta_", .)) %>% 
    mutate(n = n)
}
  
draws <- map_dfr(c(10, 100, 1000), draw)

draws %>% 
  ggplot() +
  geom_dotplot(aes(x = beta_Report), binwidth = .02) +
  geom_vline(xintercept = tidy(model) %>% 
               filter(term == "Report") %>% 
               .$estimate, 
             color = "red") +
  facet_grid(n ~ ., scales = "free_y")
```

---

We can thus write a function to estimate a distribution of predicted probabilities using a large number of random draws from a multivariate normal distribution, setting our model estimates as the means. 
 
In Stata:
```
* Calculation of Logit Bounds in Exercise 4 */
/* for a particular set of values of the independent variables */
/* After estimating logit, get coefficient and covariances */
/* (Important: enter variables in order below) */
/* matrix define b=e(b) */
/* matrix define V=e(V) */
/* Excute this program then execute the following */
/* Logit_bounds 1 2 3 4 */
/* where 1 is the value of report */
/* 2 is the value of convict */
/* 3 is the value of take */
/* 4 is the value of night */
/* */
/* */
capture program drop Logit_bounds
program define Logit_bounds
drop _all
drawnorm c_report c_convict c_take c_night c_cons, means(b) cov(V) cstorage(full) n(1000)
generate z=`1'*c_report+`2'*c_convict+`3'*c_take+`4'*c_night+c_cons
generate p = 1/(1+exp(-z))
sum p, d
end
```

In R:
```{r logit_bounds}
library(mvtnorm)# for multivariate normals

Logit_bounds <- function(model, Take, Report, Night, Convict){
  predictions <- rmvnorm(n = 1000, 
                         mean = model$coefficients, 
                         sigma = vcov(model)) %>%
    as_tibble() %>% 
    # Add a prefix to be clear that these are our betas
    rename_all(~str_c("beta_", .)) %>% 
    # z = log odds (the linear combination of predictors)
    mutate(z = `beta_(Intercept)` + beta_Take*Take + beta_Report*Report + beta_Night*Night + beta_Convict*Convict) %>% 
    # p = probabilty. Apply the logistic (inverse logit) function to the log odds
    mutate(p = 1/(1+exp(-z)) ) %>%
    # Add values to the data frame
    mutate(Take = Take,
           Report = Report,
           Night = Night,
           Convict = Convict)
  return(predictions)
}

# Test out our function
Logit_bounds(model = model, 
             Take = mean(d$Take),
             Report = 1, 
             Night = 1, 
             Convict = 0) %>%
  glimpse()
```

Apply this function to interesting possible values of our explainatory variables with `pmap` from [`purrr`](https://www.rstudio.com/resources/cheatsheets/#purrr): 
```{r predictions, fig.width=8, fig.height=5}
# a data frame of possible values
values <- d %>% 
  expand(Take = mean(Take), Report, Night, Convict)

# map values to Logit_bounds function, return a dataframe
predicted <- pmap_dfr(.l = values, # the values at which to run the function
                      .f = Logit_bounds, # the function
                      model = model) # an argument to the function that does not change

predicted %<>% 
  mutate(Convict = str_c(Convict, " Conviction(s)")) %>%
  mutate(Report = ifelse(Report == 1, "Report", "No Report"))

predicted %>%
  ggplot() +
  aes(x = p, 
      fill = factor(Report),
      color = factor(Report)) +
  geom_density(alpha = .5, trim = T) +
  facet_wrap(.~ Convict, 
             ncol = 1,
             scales = "free_y") +
  scale_color_discrete()
```



# More examples

[An R-bloggers post on simulating confidence intervals](https://www.r-bloggers.com/simulating-confidence-intervals/)

[A tutorial with some bias and model diagnostics](http://r-statistics.co/Logistic-Regression-With-R.html)

[Interactions in Logit with `margins`](https://cran.r-project.org/web/packages/margins/vignettes/Introduction.html#interactions_in_logit)

From a post on stack exchange: 
```{r predict}
df <- read.csv("https://sciences.ucf.edu/biology/d4lab/wp-content/uploads/sites/125/2018/11/parasites.txt", header = T)

m1 <- glm(data=df, infected ~ age + weight + sex, family = "binomial") # add spaces to variables separated by arithmetic operators
link_func <- m1$family$linkinv # maybe this could become a generic function

# anonymous functions are quick and easy to type, my preference if only one input arg
newdat_func <- . %>% # meant to start with df
  dplyr::select(weight, age) %>% # keep only column of interest
  map(~ round(seq(min(.), max(.), length.out = 15))) %>% # don't repeat yourself and call the same operation on both columns in one line
  c(list(sex = c("female", "male"))) %>% # prep a 3-element list for expand.grid to process
  expand.grid()

newdat2 <- newdat_func(df)

# fall back to traditional function format for multiple inputs
x_func <- function(model, newdata, link_func) {
  predict.glm(model, newdata = newdata, type="link", se=TRUE) %>% # obviously this only works on glm objects, you could add checks to be defensive
    keep(~ length(.) == nrow(newdata)) %>% # drop the third element that is length 1
    bind_cols() %>% # build data frame with a column from each list element
    mutate(low = fit - 1.96 * se.fit,
           high = fit + 1.96 * se.fit) %>%
    mutate_all(funs(link_func)) %>% # again don't repeat yourself
    bind_cols(newdata) %>% # bolt back on simulated predictors
    mutate(category = cut(age,
                          breaks = c(0, 69, 138, 206),
                          labels = c("0-69", "70-139", "139-206")),
           age = as.factor(age))
}

x2 <- x_func(m1, newdat2, link_func)

ggplot(data = x2, aes(x = weight)) + # always use spaces around '+' and '=', do ggplot(data = data) +
  geom_line(aes(y = fit, color = age)) +
  geom_ribbon(aes(ymin = low, ymax = high, fill = age), alpha = 0.1) + # okay is all on one line (<80 chars)
  facet_grid(category ~ sex) 
```