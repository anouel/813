---
title: "Questions about logit"
output:
  html_document:
    code_folding: hide
    toc: true
    #toc_float: true
---


```{r setup, include=FALSE}
# Sets defaults for R chunks
knitr::opts_chunk$set(echo = TRUE, # echo = TRUE means that your code will show
                      warning=FALSE,
                      message=FALSE,
                      # fig.path='Figs/', # where to save figures
                      fig.height = 3,
                      fig.width = 4,
                      fig.align = 'center')

# Add any R packages you require. 
# Here are some we will use in 811:
requires <- c("tidyverse", # tidyverse includes dplyr and ggplot2
              "broom",
              "haven",
              "devtools",
              "magrittr",
              "mvtnorm",
              "gridExtra",
              "here")

# Install any you don't have
to_install <- c(!requires %in% rownames(installed.packages()))
install.packages(c(requires[to_install], "NA"), repos = "https://cloud.r-project.org/" )

# Load all required R packages
library(tidyverse)
library(broom)
library(haven)
library(magrittr)
library(here)
library(mvtnorm)# for multivariate normals
library(gridExtra)
library(ggplot2); theme_set(theme_bw()) # global plot theme
# Colorblind and greyscale print-friendly scales
  options(
    ggplot2.continuous.color = "viridis",
    ggplot2.continuous.fill = "viridis"
  )
  scale_color_discrete <- function(...)
    scale_color_viridis_d(..., begin = 0, end = .7)
  scale_fill_discrete <- function(...)
    scale_fill_viridis_d(..., begin = 0, end = .7)
```

# How do we model logit in R?
## Logit is just a linear model predicting log odds. 

```{r model}
d <- read_dta("data/EX4.dta") %>% zap_formats() 

model <- glm(Probat ~ Take + Report + Night + Convict, 
             data=d, 
             family=binomial(link="logit"))

knitr::kable(tidy(model), digits = 3)
```

# How do we interpret a logit?

Interpreting log odds is tricky. Thus we transform the estimated log odds, $\hat{Y}$, into predicted probabilities at relevant potential values of our variables. Log odds can be converted into proportions/probabilities using the inverse logit function:

$p = e^\hat{Y} / 1+e^\hat{Y}$

But most people have a hard time doing this in their head. A logistic regression intercept of 4 corresponds to odds of $e^2=$ `r exp(4)`, meaning that probation was about `r round(exp(4))` times more likely than no probation for the reference group (0 convictions, no report, daytime crime, 0$ taken, etc.). However, interpreting other model coefficients is not as straightforward. This is because the simple inverse function calculates log odds relative to 0, whereas the regression coefficients are relative to the log odds of everything else in the regression including the intercept. To get a meaningful estimate of the change in probability, we have to run the logit function on the predicted value for each condition (i.e., plugging the coefficients into the regression equation). *Differences in predicted probabilities (e.g., between report and no report) depend on the value of the other coefficients*.

With linear OLS regression, model coefficients have a straightforward interpretation: a model coefficient $\beta$ means that for every one-unit increase in $x$, the model predicts a $\beta$-unit increase in $\hat{Y}$  (the predicted value of the dependent variable). Technically, the logistic regression coefficient means the same thing: as $x$ goes up by 1, the log odds go up by $\beta$. However, log odds are difficult to interpret, especially *relative log odds*: while we can straightforwardly calculate $\beta$ log odds on its own, an *increase* of $\beta$ in log odds means something different *depending on what the log odds increased from*.

tl;dr, interpreting logit means calculating *marginal* effects and *predicted probabilities*

(just like [last week](https://judgelord.github.io/813/DE3.html#better_interpretation))

## Predicted probability

`augment()` gives us predicted outcomes and standard errors. The `newdata` argument specifies values at which we want to predict. The default fitted value is on the scale of the linear predictors. `type.predict = "response"` applies the link function to the fitted values (for logit, this means transforming *log odds* into *predicted probability*).
```{r augment, fig.height=4}
# A data frame of values at which to estimate probabilities:
values <- d %>% 
  expand(Take = mean(Take), Report, Night = 1, Convict)

predicted <- augment(model,  
                     type.predict = "response",
                     newdata = values
                     ) 

# Naming things a bit better
predicted %<>% 
  mutate(Convict = str_c(Convict, " Conviction(s)")) %>%
  rename(Convictions = Convict) %>% 
  mutate(Report = ifelse(Report == 1, "Report", "No Report"))

# As a table
predicted %>% 
  select(Report, Convictions, .fitted) %>%
  mutate(.fitted = round(.fitted, 1)) %>% 
  group_by(Convictions, Report) %>%
  spread(key = Report, value = ".fitted") %>% 
  knitr::kable(caption = "Probability of probation")

# As a plot
predicted %>% 
  ggplot() + 
  aes(x = Report, y = .fitted, color = Report) + 
  geom_pointrange(aes(ymin = .fitted - 1.96*.se.fit,
                  ymax = .fitted + 1.96*.se.fit) )  + 
  coord_flip() +
  facet_wrap("Convictions", ncol = 1) +
  labs(y = "Probability", x = "") + 
  scale_color_discrete() + 
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())
```

# How can we calculate the uncertainty in these estimates?

Instead of relying on functions like `augment`, `Zelig`, or `margins`, we can estimate uncertainty more directly by sampling the assumed distribution of our parameters.

The central limit theorem shows that, with a large enough sample and bounded variance, we can simulate the distribution of parameters with draws from a multivariate normal distribution.

Consider draws of 10, 100, and 1000 for one paramater (i.e., one dimension of our multivariate normal):
```{r multi-normal-1, fig.height=4, fig.width=8}
library(mvtnorm)# for multivariate normals

draw <- function(n){
  # rnorm(n) returns n draws from a normal with a given mean and sd
  # mvrnorm(n) returns n draws from a multivariate normal with a given a vector of means (here, our betas) and sigma, a covariance matrix (here, the estimated covariances between the parameter estimates)
rmvnorm(n = n, 
        mean = model$coefficients, 
        sigma = vcov(model)) %>%
    as_tibble() %>% 
    rename_all(~str_c("beta_", .)) %>% 
    mutate(n = n)
}
  
draws <- map_dfr(c(10, 100, 1000), draw)

draws %>% 
  ggplot() +
  geom_dotplot(aes(x = beta_Report), binwidth = .02) +
  geom_vline(xintercept = tidy(model) %>% 
               filter(term == "Report") %>% 
               .$estimate, 
             color = "blue") +
  facet_grid(n ~ ., scales = "free_y") + 
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())
```

Now each draws on two parameters (i.e., two dimensions of our multivariate normal):
```{r multi-normal-2, fig.width=8}
p <- draws %>% 
  filter(n == 1000) %>% 
  ggplot() +
  aes(x = beta_Report, y = beta_Convict) +
  geom_point()

p +   geom_vline(xintercept = tidy(model) %>% 
               filter(term == "Report") %>% 
               .$estimate, 
             color = "blue")

p +
  geom_rug(alpha = .2, color = "blue") + 
  # Report (beta_3) mean, variance
  geom_label(aes(x = mean(beta_Report), 
                 y = min(beta_Convict)), 
             label= str_c(round(model$coefficients[3], 1),
                          round(vcov(model),1)[3,3],
                          sep = ", "), 
             color = "blue", vjust = 0)+ 
  # Convict (beta_5) mean, variance 
  geom_label(aes(x = min(beta_Report), 
                 y = mean(beta_Convict)), 
             label = str_c(round(model$coefficients[5], 1),
                          round(vcov(model),1)[5,5],
                          sep = ", "), 
             color = "blue", hjust = 0)

p + geom_smooth(se = F, method = "lm") + # (just to illustrate, not exactly the covariance)
  geom_label(aes(x = mean(beta_Report), 
                 y = mean(beta_Convict)), 
             label= round(vcov(model),1)[3,5], 
             color = "blue")

# Select the estimated covariances between the parameter estimates for Report and Convict
vcov(model)[c(3,5),c(3,5)] %>% 
  round(1) %>% 
  knitr::kable(caption = "Variance-Covariance Matrix")
```

Imagining a multivariate (multi-dimensional) distribution with more than two parameters can be tricky, but to understand uncertianty in logit, we must understand the relationship between *covarience* and predicted probability. 


We can thus write a function to estimate a distribution of predicted probabilities using a large number of random draws from the assumed multivariate normal distribution of our model estimates. That is, we draw from a multivariate normal distribution, setting the means to our model estimates and $\sigma$ to the variance-covariance matrix, then calculate the log odds, and, finally, a predicted probability of probation for each draw.
 
In Stata:
```
* Calculation of Logit Bounds in Exercise 4 */
/* for a particular set of values of the independent variables */
/* After estimating logit, get coefficient and covariances */
/* (Important: enter variables in order below) */
/* matrix define b=e(b) */
/* matrix define V=e(V) */
/* Excute this program then execute the following */
/* Logit_bounds 1 2 3 4 */
/* where 1 is the value of report */
/* 2 is the value of convict */
/* 3 is the value of take */
/* 4 is the value of night */
/* */
/* */
capture program drop Logit_bounds
program define Logit_bounds
drop _all
drawnorm c_report c_convict c_take c_night c_cons, means(b) cov(V) cstorage(full) n(1000)
generate z=`1'*c_report+`2'*c_convict+`3'*c_take+`4'*c_night+c_cons
generate p = 1/(1+exp(-z))
sum p, d
end
```

In R:
```{r logit_bounds, show = TRUE}

Logit_bounds <- function(model, Take, Report, Night, Convict){
  predictions <- rmvnorm(n = 1000, 
                         mean = model$coefficients, 
                         sigma = vcov(model)) %>%
    as_tibble() %>% 
    # Add a prefix to be clear that these are our betas
    rename_all(~str_c("beta_", .)) %>% 
    # z = log odds (the linear combination of predictors)
    mutate(z = `beta_(Intercept)` + beta_Take*Take + beta_Report*Report + beta_Night*Night + beta_Convict*Convict) %>% 
    # p = probabilty. Apply the logistic (inverse logit) function to the log odds
    mutate(p = 1/(1+exp(-z)) ) %>%
    # Add values to the data frame
    mutate(Take = Take,
           Report = Report,
           Night = Night,
           Convict = Convict)
  return(predictions)
}
```

First, let's test out our function:
```{r}
Logit_bounds(model = model, 
             Take = mean(d$Take),
             Report = 1, 
             Night = 1, 
             Convict = 0) %>%
  glimpse()
```

We can apply this function to interesting possible values of our explainatory variables with `pmap` from [`purrr`](https://www.rstudio.com/resources/cheatsheets/#purrr): 
```{r predictions, fig.height=5}
# a data frame of possible values
values <- d %>% 
  expand(Take = mean(Take), Report, Night, Convict)

# map values to Logit_bounds function, return a dataframe
predicted <- pmap_dfr(.l = values, # the values at which to run the function
                      .f = Logit_bounds, # the function
                      model = model) # an argument to the function that does not change

predicted %<>% 
  mutate(Convict = str_c(Convict, " Conviction(s)")) %>%
  mutate(Report = ifelse(Report == 1, "Report", "No Report"))

predicted %>%
  ggplot() +
  aes(x = p, 
      fill = Report,
      color = Report) +
  geom_density(alpha = .5, trim = T) +
  facet_wrap(.~ Convict, 
             ncol = 1,
             scales = "free_y") +
  scale_color_discrete()+ 
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())
```

Look familiar? 

To get distributions of predicted probability at specific values, you can either run the function on lots of values of interest and subset (like the plot above does) or run the function on just one set of values:

```{r}

```

# How do we interpret the results? What are logit bounds?

Devin: Good question. "Logit_bounds" may not be the best name for the above function. "Logit_distributions" or "Logit_predictions" might be better, since it does not (yet) calculate bounds (i.e. confidence intervals). What it does do is create distributions with a mean and, critically, *variance*, for predicted probabilities of interest. My understanding is that Dave wants you to use these distributions to estimate confidence intervals around certain probabilities of interest in order comment on the extent to which the department should continue writing reports (i.e., under what conditions, if any, can we say these reports matter). 


# What are the probabilities of interest? 

Devin: This is a theoretical question. What might policymakers care about?

# Can we just use `confint`?

Devin: Yes, in practice, we just use functions under the hood of `augment()`, `Zelig`, `margins`, and `confit()`. The point of this exercise is to understand the concept of a confidence interval around a predicted probability coming from a logit model. Unlike linear regression, we can't just calculate standard errors from variance. Notice that [`confint`](https://www.rdocumentation.org/packages/stats/versions/3.5.3/topics/confint) uses simple $t$ values for models of class `"lm"`, but it requires the variance-covariance matrix for `"glm"` and calls on more complex functions to compute the [profile likelihood](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3873110/). In short, [the math for Logit is not easy](https://freakonometrics.hypotheses.org/52747); it is easier to understand by looking at how the assumed distribution of the parameters translates to the distribution of probabilities.


# How do we run the following commands from Stata in R? `estat gof`, `estat classification`, `lroc`, `lsens`.

Devin: All of these functions return statistics about a model; the latter three about how well it predicts observed outcomes. 

[`estat`](https://www.reed.edu/psychology/stata/analyses/parametric/Regression/pe/estat.html) returns various model statistics. For many of these statistics, we might use `summary()`, `glance()`, `augment()`, `vcov()` depending on what exactly we are after. 

`estat classfication` returns a [classification table](https://stackoverflow.com/questions/13661025/classification-table-for-logistic-regression-in-r). 

[`lroc`](https://stats.idre.ucla.edu/stata/faq/how-can-i-test-the-difference-in-area-under-roc-curve-for-two-logistic-regression-models/) computes the area under the receiver
operating character (ROC) curve and [`lsens`](https://www.stata.com/manuals13/rlsens.pdf) runs a sensitivity analysis.  I know `ROCR` and [others](https://rviews.rstudio.com/2019/03/01/some-r-packages-for-roc-curves/) do both of these things.

`estat gof` is a goodness of fit test, specifically the F-test [(recall last week)](https://judgelord.github.io/813/DE3.html#comparing_models). 


# When should we do sensitivity analyses? 

Devin: We do a sensitivity analysis to evaluate a classification method. Any classification method has a sensitivity/specificity tradeoff; some give relatively more false negatives and others more false positives. Before we can evaluate our classification, we need to do some classifying. Let's start by classifying each case that our model gave probability less than .5 as a (no probation) and the rest a 1 (probation). We can just round our predicted probabilities. 

```{r}
predicted <- augment(model,  
                     type.predict = "response"
                     ) %>% 
  mutate(classification = round(.fitted)) %>% 
  mutate(type = ifelse(classification == 0 & Probat == 0, "True negative",
                ifelse(classification == 1 & Probat == 1, "True positive",
                ifelse(classification == 1 & Probat == 0, "False positive",
                "False negative")))
         )

predicted %>% select(Probat, .fitted, classification, type)

table <- predicted %>% group_by(type) %>%
  count() 
table
```

Specifictity = 
True Positive / Positive = 
Correctly predicted probation / probation = 
`r round(table$n[4]/(table$n[1]+table$n[4]),2)`

Sensitivity (recall) = 
True Negative / Negative = 
Correctly predicted no probation / no probation = 
`r round(table$n[3]/(table$n[2]+table$n[3]),2)`

# How do we interpret sensitivity analyses?
An ROC curve measures these statistics at different possible cut-off points. 
```{r ROC, fig.width=8}
# devtools::install_github("sachsmc/plotROC")
library(plotROC)

predicted %>% 
  mutate(Report = ifelse(Report == 1, "Report", "No Report")) %>%
  ggplot() + 
  geom_roc(aes(d = Probat, m = .fitted)) + 
  style_roc() + 
  facet_wrap("Report")
```

One use for this sensitivity-specificity analysis is choosing a cut-off point (e.g., for decision-making or for assigning cases to groups). 

Another is to assess model performance. If a model that performs well across possible cut-off points, the ROC curve will hug the upper-left corner of the plot. *Area Under the Curve* (AUC) is a measure of model performance. 


# More resources

[A tutorial on interpreting Logit coeficients](http://www.mypolyuweb.hk/~sjpolit/logisticregression.html)

[An R-bloggers post on simulating confidence intervals](https://www.r-bloggers.com/simulating-confidence-intervals/)

[A tutorial with some model diagnostics](http://r-statistics.co/Logistic-Regression-With-R.html)

[Interactions in Logit with `margins`](https://cran.r-project.org/web/packages/margins/vignettes/Introduction.html#interactions_in_logit)

Logistic Regression is core to deep learning, or, conversely [logistic regression can be viewed as a simple kind of neural network](https://beamandrew.github.io/deeplearning/2017/02/23/deep_learning_101_part2.html)

[Logistic Regression with a Neural Networks Mindset](https://edorado93.github.io/2018/09/07/Logistic-Regression-with-a-Neural-Networks-Mindset-9b5526c2ed46/)

[A nice walk-through of classification with logit](https://freakonometrics.hypotheses.org/52747)

[A great video tutorial explaining Support Vector Machines ](https://www.youtube.com/watch?v=N1vOgolbjSc)

From a post on stack exchange: 
```{r predict, fig.width=8, fig.height=5}
df <- read.csv("https://sciences.ucf.edu/biology/d4lab/wp-content/uploads/sites/125/2018/11/parasites.txt", header = T)

m1 <- glm(data=df, infected ~ age + weight + sex, family = "binomial") # add spaces to variables separated by arithmetic operators
link_func <- m1$family$linkinv # maybe this could become a generic function

# anonymous functions are quick and easy to type, my preference if only one input arg
newdat_func <- . %>% # meant to start with df
  dplyr::select(weight, age) %>% # keep only column of interest
  map(~ round(seq(min(.), max(.), length.out = 15))) %>% # don't repeat yourself and call the same operation on both columns in one line
  c(list(sex = c("female", "male"))) %>% # prep a 3-element list for expand.grid to process
  expand.grid()

newdat2 <- newdat_func(df)

# fall back to traditional function format for multiple inputs
x_func <- function(model, newdata, link_func) {
  predict.glm(model, newdata = newdata, type="link", se=TRUE) %>% # obviously this only works on glm objects, you could add checks to be defensive
    keep(~ length(.) == nrow(newdata)) %>% # drop the third element that is length 1
    bind_cols() %>% # build data frame with a column from each list element
    mutate(low = fit - 1.96 * se.fit,
           high = fit + 1.96 * se.fit) %>%
    mutate_all(funs(link_func)) %>% # again don't repeat yourself
    bind_cols(newdata) %>% # bolt back on simulated predictors
    mutate(category = cut(age,
                          breaks = c(0, 69, 138, 206),
                          labels = c("0-69", "70-139", "139-206")),
           age = as.factor(age))
}

x2 <- x_func(m1, newdat2, link_func)

ggplot(data = x2, aes(x = weight)) + # always use spaces around '+' and '=', do ggplot(data = data) +
  geom_line(aes(y = fit, color = age)) +
  geom_ribbon(aes(ymin = low, ymax = high, fill = age), alpha = 0.1) + # okay is all on one line (<80 chars)
  facet_grid(category ~ sex) 
```